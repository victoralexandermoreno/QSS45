{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b081e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "peng = load_penguins()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c7748",
   "metadata": {},
   "source": [
    "#### Pre-processing data\n",
    "* Using pd.factorize(), transform the categorical variables into numbers.\n",
    "* Afterward, impute the missing values for each column by the mean.\n",
    "* Then, using train_test_split, generate a 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3de649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2abb44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d619c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class penguinNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(penguinNet, self).__init__()\n",
    "        #your architecture\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # your forward solution\n",
    "        # note the last pass does not need to go through activation / non-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae74a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters / You can vary this as you wish\n",
    "input_size = 7\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    "model = penguinNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # your training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f0cd9",
   "metadata": {},
   "source": [
    "#### Run this to evaluate your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Forward pass on the test set\n",
    "    outputs = model(X_test_tensor)\n",
    "    \n",
    "    # Get predicted classes\n",
    "    _, ypred = torch.max(outputs, 1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    total = y_test_tensor.size(0)\n",
    "    correct = (ypred == y_test_tensor).sum().item()\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c6392",
   "metadata": {},
   "source": [
    "### Generate a confusion matrix of the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8f4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64b4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695fdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78f84d12",
   "metadata": {},
   "source": [
    "# Word2Vec: The Prehistoric LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2bc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bbcdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data files\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22fa03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "sentences = []\n",
    "for fileid in reuters.fileids():\n",
    "    words = reuters.words(fileid)\n",
    "    sentences.append([word.lower() for word in words if word.isalpha()])\n",
    "\n",
    "# Display a few example sentences\n",
    "print(\"Example sentences:\")\n",
    "for i in range(2):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=40, \n",
    "                 window=5, \n",
    "                 min_count=5, \n",
    "                 sg=1, \n",
    "                 epochs=100)\n",
    "\n",
    "# Display vocabulary size\n",
    "print(\"Vocabulary size:\", len(model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding words similar to 'market'\n",
    "similar_words = model.wv.most_similar('market', topn=10)\n",
    "print(\"Words similar to 'market':\")\n",
    "\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a subset of words for visualization\n",
    "words = ['market', 'economy', 'trade', 'finance', 'investment', 'growth', 'bank', 'money', 'stocks', 'currency']\n",
    "word_vectors = np.array([model.wv[word] \n",
    "                         for w in words \n",
    "                         if w in model.wv])\n",
    "\n",
    "# Reduce dimensions with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=2,)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the words and their vectors\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
    "\n",
    "# Annotate each point with the word\n",
    "for i, word in enumerate(words):\n",
    "    if word in model.wv:\n",
    "        plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
    "\n",
    "plt.title(\"Word Embedding Visualization with t-SNE\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c76ae6",
   "metadata": {},
   "source": [
    "## Exploring word2vec\n",
    "* First, use two of your favorite word and identify the top 40 similar words.\n",
    "* Then transform those words into vectors.\n",
    "* Using Principal component analysis, plot the 80 words using MatPlotLib, coloring the two word groups differently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9078a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31002821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c30900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abeff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
