{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c552abf",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa609da",
   "metadata": {},
   "source": [
    "The goal of this workbook is to get a broad sense of how Q-learning works.\n",
    "Scenario: Moo Deng (the pygmy hippo) is trying to escape the zoo!\n",
    "As an aspiring QSS major, she is trying to simulate her escape using Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b223f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fade45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGridEnv:\n",
    "    def __init__(self, size=5, goal=(4, 4), pitfall=(2, 2)):\n",
    "        self.size = size\n",
    "        self.goal = goal\n",
    "        self.pitfall = pitfall\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        row, col = self.agent_pos\n",
    "        \n",
    "        # Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "        if action == 0 and row > 0:  # Up\n",
    "            # your code | Think of what to add or minus to your row and column\n",
    "            \n",
    "        elif action == 1 and row < self.size - 1:  # Down\n",
    "            # your code\n",
    "            \n",
    "        elif action == 2 and col > 0:  # Left\n",
    "            # your code\n",
    "            \n",
    "        elif action == 3 and col < self.size - 1:  # Right\n",
    "            # your code\n",
    "        \n",
    "        self.agent_pos = (row, col)\n",
    "        \n",
    "        # Determine rewards\n",
    "        # This block of code returns the agent's position, a reward value, and termination\n",
    "        if self.agent_pos == self.goal:\n",
    "            return self.agent_pos, 1, True  # Reward of +1 for reaching the goal\n",
    "        elif self.agent_pos == self.pitfall:\n",
    "            return self.agent_pos, # your reward, # your ending condition\n",
    "        else:\n",
    "            return self.agent_pos, # your reward, # your ending condition \n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((self.size, self.size), ' ')\n",
    "        grid[self.goal] = 'G'       # Goal\n",
    "        grid[self.pitfall] = 'X'    # Pitfall\n",
    "        grid[self.agent_pos] = 'M'  # Agent Moo Deng's position\n",
    "        print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((env.size, env.size, 4))  # Q-table for each state-action pair\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            ####\n",
    "            # Your code for Explore: generate a random action by sampling the possible actions\n",
    "            ####\n",
    "        else:\n",
    "            row, col = state\n",
    "            return np.argmax(self.q_table[row, col])  # Exploit: best action based on Q-table\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        row, col = state\n",
    "        next_row, next_col = next_state\n",
    "        best_future_q = np.max(self.q_table[next_row, next_col])\n",
    "        \n",
    "        # Q-learning update rule | Bellman Equation\n",
    "        self.q_table[row, col, action] = (1 - self.learning_rate) * self.q_table[row, col, action] + \\\n",
    "            self.learning_rate * (reward + self.discount_factor * best_future_q)\n",
    "    \n",
    "    def train(self, episodes=50):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            print(f\"\\nEpisode {episode + 1}/{episodes}\")\n",
    "            while not done:\n",
    "                clear_output(wait=True)\n",
    "                self.env.render()\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                self.update_q_value(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                time.sleep(0.3)\n",
    "            print(f\"Episode finished in {steps} steps\")\n",
    "            time.sleep(3)\n",
    "\n",
    "# Interactive Demo\n",
    "def interactive_q_learning_demo():\n",
    "    print(\"Welcome to the Q-Learning Demo! Help Moo Deng Escape!\")\n",
    "    size = int(input(\"Enter grid size (e.g., 5): \"))\n",
    "    learning_rate = float(input(\"Enter learning rate (α) (e.g., 0.1): \"))\n",
    "    discount_factor = float(input(\"Enter discount factor (γ) (e.g., 0.9): \"))\n",
    "    epsilon = float(input(\"Enter exploration rate (ε) (e.g., 0.1): \"))\n",
    "    episodes = int(input(\"Enter number of training episodes: \"))\n",
    "    \n",
    "    env = SimpleGridEnv(size=size)\n",
    "    agent = QLearningAgent(env, learning_rate=learning_rate, discount_factor=discount_factor, epsilon=epsilon)\n",
    "    \n",
    "    print(\"Training Moo Deng...\")\n",
    "    agent.train(episodes)\n",
    "    print(\"Training complete! Here is the final Q-table:\")\n",
    "    print(agent.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb0c5ac",
   "metadata": {},
   "source": [
    "#### Once you have filled in the two code blocks, try running the demo with 3 training episodes\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the interactive demo\n",
    "interactive_q_learning_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dfad56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b8457d8",
   "metadata": {},
   "source": [
    "#### Now try running it for many more episodes (adjust sleep for faster training)\n",
    "- What do you observe about the final q-tables?\n",
    "- Afterward, experiment with a) the rewards, b) grid-size, and c) exploration\n",
    "- What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e43502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490d496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d92507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51352f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
